{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMtv2nmo2dru1KEPxDIhdE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#mounting the google drive for accessing datasets and modelFile as well.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pjHPPKpYqXit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069fdd7a-70d7-4ca7-f47d-6b60cfc46558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RFdm9ygQqXfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwZiCCrtUwO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc0fec3-3a68-4028-932b-4a7c12791ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 118MB/s]\n"
          ]
        }
      ],
      "source": [
        "#importing all the necessary libraries and dependencies\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of classes (KITTI has 8 classes including background)\n",
        "num_classes = 8\n",
        "\n",
        "# Get the number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the head with a new one\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Ensure the final layer's parameters are trainable (not strictly necessary, but ensures correctness)\n",
        "for param in model.roi_heads.box_predictor.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "bPrdqZZ5U1PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Datasets Configuration\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "#Defining the class for loading, and transforming the datasets to the appropriate configuration to which the model accepts\n",
        "class KITTIDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        # Initialize the dataset with the root directory and transformations\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # List of image file names\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "        # List of label file names\n",
        "        self.anns = list(sorted(os.listdir(os.path.join(root, \"labels\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load an image and its corresponding bounding boxes\n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        ann_path = os.path.join(self.root, \"labels\", self.anns[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Parse(analyse) the annotation file\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = f.readlines()\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for line in data:\n",
        "            parts = line.strip().split()\n",
        "            class_name = parts[0]\n",
        "            bbox = list(map(float, parts[4:8]))  # Extract bounding box coordinates\n",
        "\n",
        "            if class_name in KITTI_CLASSES:\n",
        "                labels.append(KITTI_CLASSES.index(class_name))  # Convert class name to index\n",
        "                boxes.append(bbox)\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Create a target dictionary\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        # Apply transformations to the image\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of images\n",
        "        return len(self.imgs)\n",
        "\n",
        "    # def to_dataframe(self):\n",
        "    #     data = []\n",
        "    #     for idx in range(len(self)):\n",
        "    #         ann_path = os.path.join(self.root, \"labels\", self.anns[idx])\n",
        "    #         with open(ann_path, 'r') as f:\n",
        "    #             annotation = f.readlines()\n",
        "\n",
        "    #         for line in annotation:\n",
        "    #             parts = line.strip().split()\n",
        "    #             class_name = parts[0]\n",
        "    #             bbox = list(map(float, parts[4:8]))\n",
        "\n",
        "    #             if class_name in KITTI_CLASSES:\n",
        "    #                 data.append([self.imgs[idx], class_name] + bbox)\n",
        "\n",
        "    #     df = pd.DataFrame(data, columns=['image', 'class', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
        "    #     return df\n",
        "\n",
        "\n",
        "# Define the class names as per KITTI dataset\n",
        "KITTI_CLASSES = ['__background__', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram']\n",
        "\n",
        "def get_transform():\n",
        "    # Define the transformations to apply\n",
        "    transforms = [\n",
        "        # T.Resize((800, 1333)),  # Resize the image to (800, 1333)\n",
        "        T.ToTensor(),  # Convert PIL image to tensor\n",
        "        # T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n",
        "    ]\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "# Create the dataset\n",
        "dataset = KITTIDataset(root='/content/drive/MyDrive/Object Detection of Images CV Project', transforms=get_transform())\n",
        "\n",
        "\n",
        "# df = dataset.to_dataframe()\n",
        "# print(df.head())\n",
        "\n",
        "\n",
        "#Testing the Dataset\n",
        "img, target = dataset[0]  # Load the first image and its target\n",
        "print(img.shape)  # Should print the shape of the image tensor\n",
        "# print(f\"Image ID: {image_id}\")\n",
        "print(target)  # Should print the target dictionary containing boxes and labels\n",
        "\n"
      ],
      "metadata": {
        "id": "jwZR_d2XU1Bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06205137-56c5-4135-94f3-47a63cca09e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 370, 1224])\n",
            "{'boxes': tensor([[712.4000, 143.0000, 810.7300, 307.9200]]), 'labels': tensor([4])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train_Test split and DataLoaders\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Split dataset into train and test\n",
        "#Out of 3000 images and Labels dataset, 2400 of them will be used for training and 800 will be used for evaluating or testing\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "dataset_train, dataset_test = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define data loaders\n",
        "data_loader_train = DataLoader(dataset_train, batch_size=32, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
        "data_loader_test = DataLoader(dataset_test, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "keBXROyhqWg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fdf970a-9f89-417d-ee57-dd7526b5a492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Visualization Function to track and view the progression of training\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def visualize_image(image, target=None, prediction=None):\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "    image = image.permute(1, 2, 0)  # Convert from [C, H, W] to [H, W, C]\n",
        "    ax.imshow(image.cpu().numpy())\n",
        "\n",
        "    if target is not None:\n",
        "        for box in target['boxes']:\n",
        "            x, y, w, h = box.cpu().numpy()\n",
        "            rect = patches.Rectangle((x, y), w-x, h-y, linewidth=2, edgecolor='green', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "    if prediction is not None:\n",
        "        for box in prediction['boxes']:\n",
        "            x, y, w, h = box.cpu().numpy()\n",
        "            rect = patches.Rectangle((x, y), w-x, h-y, linewidth=2, edgecolor='red', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "pZKgLlyQOxwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train.py\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Move model to the appropriate device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "save_path = '/content/drive/MyDrive/Object Detection of Images CV Project'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Initialize tqdm progress bar for viewing the training progress\n",
        "    pbar = tqdm(enumerate(data_loader_train), total=len(data_loader_train), desc=f'Epoch {epoch + 1}/{num_epochs}', leave=True) #pbar -> progress bar\n",
        "\n",
        "    for batch_idx, (images, targets) in pbar:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets) #Perform a forward pass and compute the loss.\n",
        "        losses = sum(loss for loss in loss_dict.values()) #compute the total loss\n",
        "        losses.backward() #perform backpropagation to compute the gradients\n",
        "        optimizer.step() #update the model params based on the computed grads\n",
        "\n",
        "        running_loss += losses.item()\n",
        "\n",
        "        # Update tqdm progress bar\n",
        "        pbar.set_postfix({'loss': running_loss / (batch_idx + 1)})\n",
        "\n",
        "        # Visualize the first batch of each epoch\n",
        "        if batch_idx == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad(): #Perform inference without computing gradients.\n",
        "                predictions = model(images)\n",
        "            for i in range(len(images)):\n",
        "                visualize_image(images[i], target=targets[i], prediction=predictions[i])\n",
        "            model.train()\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(data_loader_train)}\")\n",
        "\n",
        "    # Save model checkpoint after each epoch\n",
        "    torch.save(model.state_dict(), os.path.join(save_path, f\"fasterrcnn_resnet50_fpn_epoch_{epoch + 1}.pth\"))\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "9I2dLKEkqWdZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "438d738b-129d-43be-a0e8-81482b5b8d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   0%|          | 0/76 [00:09<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a5f421b2b596>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/{num_epochs}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEhPKm9AsMBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing.py\n",
        "###############\n",
        "#NOTE: Please don't be confused because I have written some code segments repeatedly like: Dataset Loading and Model Loading\n",
        "#because some error came and i have to do it like this\n",
        "##################\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.transforms as T\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Define KITTI_CLASSES if not defined elsewhere\n",
        "KITTI_CLASSES = ['__background__', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram']\n",
        "\n",
        "# Define metric calculation functions\n",
        "def calculate_metrics(true_labels, pred_labels):\n",
        "    precision = precision_score(true_labels, pred_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, pred_labels, average='weighted')\n",
        "    f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Function to apply NMS (this is necessary to get only single bounding boxes) NMS-> non-maximum suppression\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "    final_prediction = {}\n",
        "    final_prediction['boxes'] = orig_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = orig_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = orig_prediction['labels'][keep]\n",
        "    return final_prediction\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "            nms_outputs = [apply_nms(output) for output in outputs]\n",
        "\n",
        "            for target, nms_output in zip(targets, nms_outputs):\n",
        "                true_labels = target[\"labels\"].tolist()\n",
        "                pred_labels = nms_output[\"labels\"].tolist()\n",
        "\n",
        "                # Extend the lists only if there are predictions\n",
        "                if pred_labels:\n",
        "                    all_true_labels.extend(true_labels)\n",
        "                    all_pred_labels.extend(pred_labels)\n",
        "\n",
        "    # Ensure that the lengths are the same\n",
        "    min_len = min(len(all_true_labels), len(all_pred_labels))\n",
        "    all_true_labels = all_true_labels[:min_len]\n",
        "    all_pred_labels = all_pred_labels[:min_len]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy, precision, recall, f1 = calculate_metrics(all_true_labels, all_pred_labels)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Load the trained model (make sure the model path is correct)\n",
        "model_path = '/content/drive/My Drive/Object Detection of Images CV Project/Best_Final_Model.pth'\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def load_model(model_path, device):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    num_classes = len(KITTI_CLASSES)\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_model(model_path, device)\n",
        "\n",
        "# Dataset definition\n",
        "class KITTIDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "        self.anns = list(sorted(os.listdir(os.path.join(root, \"labels\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        ann_path = os.path.join(self.root, \"labels\", self.anns[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = f.readlines()\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for line in data:\n",
        "            parts = line.strip().split()\n",
        "            class_name = parts[0]\n",
        "            bbox = list(map(float, parts[4:8]))\n",
        "            if class_name in KITTI_CLASSES:\n",
        "                labels.append(KITTI_CLASSES.index(class_name))\n",
        "                boxes.append(bbox)\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "# Transforms\n",
        "def get_transform():\n",
        "    transforms = [\n",
        "        T.ToTensor(),\n",
        "    ]\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = KITTIDataset(root='/content/drive/MyDrive/Object Detection of Images CV Project', transforms=get_transform())\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "dataset_train, dataset_test = random_split(dataset, [train_size, test_size])\n",
        "data_loader_test = DataLoader(dataset_test, batch_size=32, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate(model, data_loader_test, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "o2bGIuDXqWaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference.py\n",
        "\n",
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torchvision.transforms as T\n",
        "import torchvision.ops as ops\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Define KITTI_CLASSES here if not defined elsewhere\n",
        "KITTI_CLASSES = ['__background__', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram']\n",
        "\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "    keep = ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "\n",
        "    final_prediction = {}\n",
        "    final_prediction['boxes'] = orig_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = orig_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = orig_prediction['labels'][keep]\n",
        "\n",
        "    return final_prediction\n",
        "\n",
        "def infer_image(image_path, model, device, threshold=0.5):\n",
        "    # Load the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Transform the image\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "\n",
        "    # Process the outputs\n",
        "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
        "    outputs = [apply_nms(output) for output in outputs]\n",
        "    boxes = outputs[0]['boxes']\n",
        "    scores = outputs[0]['scores']\n",
        "    labels = outputs[0]['labels']\n",
        "\n",
        "    # Draw the boxes and labels on the image\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    # Use the default PIL font\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "        if score > threshold:\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3) #Draw bounding box\n",
        "            # Increase the font size by scaling\n",
        "            text = f\"{KITTI_CLASSES[label]}: {score:.2f}\"\n",
        "            text_size = font.getsize(text)\n",
        "            text_position = (xmin, ymin - text_size[1]) #determine the text position\n",
        "            draw.rectangle([text_position, (xmin + text_size[0], ymin)], fill='green') #Draw Background for Text\n",
        "            draw.text(text_position, text, fill='black', font=font) #draws or writes the text at the specified position and font specifications\n",
        "\n",
        "    return image\n",
        "\n",
        "# Path to the saved model weights in Google Drive\n",
        "model_path = '/content/drive/My Drive/Object Detection of Images CV Project/Best_Final_Model.pth'\n",
        "\n",
        "# Function to load model weights\n",
        "def load_model(model_path, device):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    num_classes = 8  # Assuming 8 classes for KITTI\n",
        "\n",
        "    # Replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Load the trained model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = load_model(model_path, device)\n",
        "\n",
        "image_path = '/content/drive/My Drive/Object Detection of Images CV Project/infer_image/ktm.png'\n",
        "\n",
        "# Inference on an example image\n",
        "result_image = infer_image(image_path, model, device)\n",
        "\n",
        "# Generate a unique name for the result image\n",
        "unique_name = f\"result_image_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "result_image_save_path = os.path.join('/content/drive/My Drive/Object Detection of Images CV Project/result_image', unique_name)\n",
        "\n",
        "# Save the result image\n",
        "result_image.save(result_image_save_path)\n",
        "\n",
        "# Display the result image in Google Colab\n",
        "plt.imshow(result_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ah38aqWjqWXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4VH68ZPBKjWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQSmdAeZFVS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ht1qiF8LFVPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}